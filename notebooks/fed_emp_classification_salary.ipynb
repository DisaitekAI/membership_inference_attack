{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f43692670d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "seed = 142856\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_desc = {\n",
    "    'AGYSUB': 'Agency',\n",
    "    'LOC': 'Location',\n",
    "    'AGELVL': 'Age (bucket)',\n",
    "    'EDLVL': 'Education level',\n",
    "    'GSEGRD': 'General schedule & Equivalent grade',\n",
    "    'LOSLVL': 'Length of service (bucket)',\n",
    "    'OCC': 'Occupation',\n",
    "    'PATCO': 'Occupation category',\n",
    "    'PPGRD': 'Pay Plan & Grade',\n",
    "    'STEMOCC': 'STEM Occupation',\n",
    "    'SUPERVIS': 'Supervisory status',\n",
    "    'TOA': 'Type of appointment',\n",
    "    'WORKSCH': 'Work schedule',\n",
    "    'WORKSTAT': 'Work status',\n",
    "    'LOS': 'Average length of service',\n",
    "    'SALBUCKET': 'Salary bucket'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path    = Path('../data/')\n",
    "model_folder = Path('../models/')\n",
    "model_path   = model_folder / 'edlvl_clf_salary_bucket.pt'\n",
    "df           = pd.read_csv(data_path / 'interim' / 'fed_emp.csv')\n",
    "emb_dim      = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the nan values in columns by either adding a new category\n",
    "# or dropping the lines\n",
    "df                                   = df[~df.EDLVL.isnull()]\n",
    "df.loc[df.GSEGRD.isnull(), 'GSEGRD'] = 0\n",
    "df.loc[df.OCC.isnull(), 'OCC']       = 0\n",
    "df                                   = df[~df.SUPERVIS.isnull()]\n",
    "df                                   = df[~df.TOA.isnull()]\n",
    "df                                   = df[~df.SALARY.isnull()]\n",
    "df                                   = df[~df.LOS.isnull()]\n",
    "# Target generation, we partition the salary values in 10 equally sized\n",
    "# buckets.\n",
    "df['SALBUCKET']                      = pd.qcut(df.SALARY, np.arange(0, 1.1, .1))\n",
    "# df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset truncation to improve overfitting capabilities\n",
    "df = df.sample(10000, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data    = df.drop(['SALBUCKET', 'SALARY', 'SALLVL'], axis = 1)\n",
    "df_target  = df['SALBUCKET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['LOS']\n",
    "df_num            = df_data[numerical_columns]\n",
    "num_val_mean      = df_num.mean(axis = 0)\n",
    "num_val_std       = df_num.std(axis = 0)\n",
    "df_num            = (df_num - num_val_mean) / num_val_std\n",
    "df_cat            = df_data.drop(numerical_columns, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_encoders = {\n",
    "    col : {\n",
    "        val : i \n",
    "        for i, val in enumerate(df[col].unique())\n",
    "    }\n",
    "    for col in df_cat.columns\n",
    "}\n",
    "target_encoder = {\n",
    "    val : i \n",
    "    for i, val in enumerate(sorted(df_target.unique()))\n",
    "}\n",
    "column_order = sorted(columns_encoders.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_cat.columns:\n",
    "    df_cat[col] = df_cat[col].apply(lambda x: columns_encoders[col][x])\n",
    "df_target = df_target.map(lambda x: target_encoder[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalEmbeddings(nn.Module):\n",
    "    def __init__(self, col_order, col_encoders, col_to_emb_dim):\n",
    "        super(CategoricalEmbeddings, self).__init__()\n",
    "        self.col_order = col_order \n",
    "        self.cat_embs  = nn.ModuleDict({\n",
    "            col: nn.Embedding(len(col_encoders[col]), col_to_emb_dim[col])\n",
    "            for col in col_order\n",
    "        })\n",
    "        \n",
    "    def forward(self, cat_variables):\n",
    "        embeddings = [self.cat_embs[col](cat_variables[col]) for col in self.col_order]\n",
    "        \n",
    "        return torch.cat(embeddings, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdlvlClassifier(nn.Module):\n",
    "    def __init__(self, col_order, col_encoders, col_to_emb_dim, class_number, num_var_number, \n",
    "                 lin_size = 256, dropout_rate = 0.):\n",
    "        super(EdlvlClassifier, self).__init__()\n",
    "        self.cat_emb    = CategoricalEmbeddings(col_order, col_encoders, col_to_emb_dim)\n",
    "        sum_cat_emb_dim = sum(col_to_emb_dim.values())\n",
    "        self.linear1    = nn.Linear(sum_cat_emb_dim + num_var_number, lin_size)\n",
    "        self.linear2    = nn.Linear(lin_size, class_number)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, cat_variables, num_variables):\n",
    "        cat_embeddings = self.cat_emb(cat_variables)\n",
    "        cat_num_tensor = torch.cat([cat_embeddings, num_variables], dim = 1)\n",
    "        cat_num_tensor = self.dropout(cat_num_tensor)\n",
    "        out_linear1    = F.relu(self.dropout(self.linear1(cat_num_tensor)))\n",
    "        out_linear2    = self.linear2(out_linear1)\n",
    "        \n",
    "        return out_linear2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdlvlClassifier(\n",
       "  (cat_emb): CategoricalEmbeddings(\n",
       "    (cat_embs): ModuleDict(\n",
       "      (AGELVL): Embedding(11, 5)\n",
       "      (AGYSUB): Embedding(351, 5)\n",
       "      (EDLVL): Embedding(21, 5)\n",
       "      (GSEGRD): Embedding(16, 5)\n",
       "      (LOC): Embedding(78, 5)\n",
       "      (LOSLVL): Embedding(10, 5)\n",
       "      (OCC): Embedding(459, 5)\n",
       "      (PATCO): Embedding(7, 5)\n",
       "      (PPGRD): Embedding(384, 5)\n",
       "      (STEMOCC): Embedding(86, 5)\n",
       "      (SUPERVIS): Embedding(6, 5)\n",
       "      (TOA): Embedding(15, 5)\n",
       "      (WORKSCH): Embedding(7, 5)\n",
       "      (WORKSTAT): Embedding(2, 5)\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=71, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.0)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EdlvlClassifier(\n",
    "    column_order,\n",
    "    columns_encoders,\n",
    "    {\n",
    "        col : emb_dim\n",
    "        for col in columns_encoders\n",
    "    },\n",
    "    len(target_encoder),\n",
    "    num_var_number = df_num.shape[1]\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(\n",
    "    *[\n",
    "        torch.tensor(df_cat[col].values)\n",
    "        for col in column_order\n",
    "    ], # categorical variables in the correct order\n",
    "    torch.tensor(df_num.values, dtype = torch.float32), # numerical variables\n",
    "    torch.tensor(df_target.values, dtype = torch.int64) # target variables\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size                 = len(dataset)\n",
    "valid_prop                   = 0.2\n",
    "valid_size                   = round(valid_prop * dataset_size)\n",
    "lengths                      = [dataset_size - valid_size, valid_size]\n",
    "train_dataset, valid_dataset = random_split(dataset, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train        = True\n",
    "device       = torch.device('cuda')\n",
    "model        = model.to(device)\n",
    "epochs       = 200\n",
    "batch_size   = 2048\n",
    "optimizer    = optim.Adam(model.parameters())\n",
    "criterion    = nn.CrossEntropyLoss()\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0:3] [T] 14.78%, [V] 21.10%  2.25\n",
      "[1:3] [T] 25.52%, [V] 29.55%  2.15\n",
      "[2:3] [T] 32.94%, [V] 33.50%  2.07\n",
      "[3:3] [T] 36.35%, [V] 36.80%  1.98\n",
      "[4:3] [T] 37.81%, [V] 36.65%  1.89\n",
      "[5:3] [T] 38.50%, [V] 37.85%  1.81\n",
      "[6:3] [T] 39.71%, [V] 39.95%  1.74\n",
      "[7:3] [T] 41.17%, [V] 41.45%  1.67\n",
      "[8:3] [T] 42.58%, [V] 42.30%  1.60\n",
      "[9:3] [T] 43.54%, [V] 43.70%  1.54\n",
      "[10:3] [T] 44.34%, [V] 44.30%  1.49\n",
      "[11:3] [T] 44.85%, [V] 44.45%  1.45\n",
      "[12:3] [T] 45.58%, [V] 44.85%  1.42\n",
      "[13:3] [T] 46.49%, [V] 45.65%  1.38\n",
      "[14:3] [T] 47.51%, [V] 45.40%  1.35\n",
      "[15:3] [T] 48.12%, [V] 45.95%  1.33\n",
      "[16:3] [T] 48.77%, [V] 46.40%  1.37\n",
      "[17:3] [T] 49.35%, [V] 47.30%  1.31\n",
      "[18:3] [T] 49.83%, [V] 48.20%  1.28\n",
      "[19:3] [T] 50.52%, [V] 48.15%  1.25\n",
      "[20:3] [T] 51.15%, [V] 48.10%  1.23\n",
      "[21:3] [T] 51.74%, [V] 48.95%  1.24\n",
      "[22:3] [T] 52.06%, [V] 48.55%  1.22\n",
      "[23:3] [T] 52.71%, [V] 48.70%  1.24\n",
      "[24:3] [T] 53.04%, [V] 49.15%  1.21\n",
      "[25:3] [T] 53.48%, [V] 49.40%  1.21\n",
      "[26:3] [T] 54.31%, [V] 49.00%  1.20\n",
      "[27:3] [T] 54.59%, [V] 49.60%  1.18\n",
      "[28:3] [T] 55.05%, [V] 50.05%  1.13\n",
      "[29:3] [T] 55.52%, [V] 49.65%  1.15\n",
      "[30:3] [T] 55.85%, [V] 50.65%  1.15\n",
      "[31:3] [T] 56.36%, [V] 50.95%  1.10\n",
      "[32:3] [T] 56.89%, [V] 50.60%  1.12\n",
      "[33:3] [T] 56.94%, [V] 51.00%  1.11\n",
      "[34:3] [T] 57.16%, [V] 51.05%  1.08\n",
      "[35:3] [T] 57.85%, [V] 51.20%  1.07\n",
      "[36:3] [T] 58.02%, [V] 51.50%  1.08\n",
      "[37:3] [T] 58.44%, [V] 51.80%  1.08\n",
      "[38:3] [T] 58.52%, [V] 51.55%  1.10\n",
      "[39:3] [T] 59.00%, [V] 52.10%  1.07\n",
      "[40:3] [T] 58.94%, [V] 52.45%  1.05\n",
      "[41:3] [T] 59.40%, [V] 51.80%  1.05\n",
      "[42:3] [T] 59.60%, [V] 52.60%  1.03\n",
      "[43:3] [T] 59.85%, [V] 52.55%  1.02\n",
      "[44:3] [T] 60.19%, [V] 52.45%  1.01\n",
      "[45:3] [T] 60.29%, [V] 53.05%  1.02\n",
      "[46:3] [T] 60.58%, [V] 53.15%  1.03\n",
      "[47:3] [T] 61.30%, [V] 52.80%  0.99\n",
      "[48:3] [T] 61.26%, [V] 53.10%  1.00\n",
      "[49:3] [T] 61.56%, [V] 53.35%  0.97\n",
      "[50:3] [T] 61.73%, [V] 53.80%  1.00\n",
      "[51:3] [T] 62.25%, [V] 53.15%  0.99\n",
      "[52:3] [T] 62.45%, [V] 53.20%  0.96\n",
      "[53:3] [T] 62.55%, [V] 54.10%  0.99\n",
      "[54:3] [T] 62.77%, [V] 53.60%  0.96\n",
      "[55:3] [T] 62.86%, [V] 53.80%  0.95\n",
      "[56:3] [T] 63.23%, [V] 54.05%  0.94\n",
      "[57:3] [T] 63.71%, [V] 53.70%  0.96\n",
      "[58:3] [T] 64.01%, [V] 53.90%  0.96\n",
      "[59:3] [T] 63.90%, [V] 54.15%  0.93\n",
      "[60:3] [T] 64.29%, [V] 54.00%  0.95\n",
      "[61:3] [T] 64.66%, [V] 54.10%  0.92\n",
      "[62:3] [T] 64.60%, [V] 54.15%  0.97\n",
      "[63:3] [T] 64.99%, [V] 54.50%  0.93\n",
      "[64:3] [T] 65.33%, [V] 53.85%  0.95\n",
      "[65:3] [T] 65.47%, [V] 54.45%  0.91\n",
      "[66:3] [T] 65.51%, [V] 54.50%  0.89\n",
      "[67:3] [T] 65.80%, [V] 54.75%  0.86\n",
      "[68:3] [T] 66.03%, [V] 54.25%  0.87\n",
      "[69:3] [T] 66.14%, [V] 54.35%  0.89\n",
      "[70:3] [T] 66.41%, [V] 55.10%  0.89\n",
      "[71:3] [T] 66.81%, [V] 54.85%  0.88\n",
      "[72:3] [T] 66.91%, [V] 54.20%  0.87\n",
      "[73:3] [T] 66.91%, [V] 55.00%  0.87\n",
      "[74:3] [T] 67.35%, [V] 55.15%  0.86\n",
      "[75:3] [T] 67.50%, [V] 54.45%  0.85\n",
      "[76:3] [T] 67.29%, [V] 54.50%  0.83\n",
      "[77:3] [T] 67.86%, [V] 54.90%  0.87\n",
      "[78:3] [T] 67.83%, [V] 54.75%  0.83\n",
      "[79:3] [T] 68.17%, [V] 54.80%  0.81\n",
      "[80:3] [T] 68.34%, [V] 54.65%  0.83\n",
      "[81:3] [T] 68.47%, [V] 55.05%  0.81\n",
      "[82:3] [T] 68.70%, [V] 54.90%  0.81\n",
      "[83:3] [T] 68.85%, [V] 55.10%  0.83\n",
      "[84:3] [T] 69.17%, [V] 54.30%  0.80\n",
      "[85:3] [T] 69.16%, [V] 55.05%  0.80\n",
      "[86:3] [T] 69.58%, [V] 55.40%  0.80\n",
      "[87:3] [T] 69.59%, [V] 54.55%  0.83\n",
      "[88:3] [T] 69.51%, [V] 55.20%  0.76\n",
      "[89:3] [T] 70.04%, [V] 54.80%  0.78\n",
      "[90:3] [T] 70.49%, [V] 55.35%  0.78\n",
      "[91:3] [T] 70.53%, [V] 55.00%  0.76\n",
      "[92:3] [T] 70.49%, [V] 54.85%  0.78\n",
      "[93:3] [T] 70.79%, [V] 55.65%  0.74\n",
      "[94:3] [T] 71.05%, [V] 55.00%  0.76\n",
      "[95:3] [T] 71.25%, [V] 55.00%  0.75\n",
      "[96:3] [T] 71.28%, [V] 55.15%  0.78\n",
      "[97:3] [T] 71.49%, [V] 55.70%  0.75\n",
      "[98:3] [T] 71.96%, [V] 54.95%  0.75\n",
      "[99:3] [T] 71.69%, [V] 55.00%  0.74\n",
      "[100:3] [T] 71.96%, [V] 55.50%  0.76\n",
      "[101:3] [T] 72.06%, [V] 55.25%  0.76\n",
      "[102:3] [T] 72.29%, [V] 55.65%  0.74\n",
      "[103:3] [T] 72.54%, [V] 55.25%  0.72\n",
      "[104:3] [T] 72.86%, [V] 54.95%  0.72\n",
      "[105:3] [T] 72.59%, [V] 55.50%  0.71\n",
      "[106:3] [T] 72.72%, [V] 55.15%  0.75\n",
      "[107:3] [T] 73.01%, [V] 55.40%  0.74\n",
      "[108:3] [T] 73.09%, [V] 55.10%  0.74\n",
      "[109:3] [T] 73.28%, [V] 55.20%  0.71\n",
      "[110:3] [T] 73.58%, [V] 55.55%  0.72\n",
      "[111:3] [T] 73.90%, [V] 55.40%  0.70\n",
      "[112:3] [T] 74.05%, [V] 55.45%  0.70\n",
      "[113:3] [T] 74.29%, [V] 55.00%  0.70\n",
      "[114:3] [T] 74.35%, [V] 55.30%  0.70\n",
      "[115:3] [T] 74.28%, [V] 56.05%  0.67\n",
      "[116:3] [T] 74.71%, [V] 55.25%  0.70\n",
      "[117:3] [T] 75.08%, [V] 55.95%  0.69\n",
      "[118:3] [T] 75.10%, [V] 54.90%  0.68\n",
      "[119:3] [T] 75.24%, [V] 55.70%  0.69\n",
      "[120:3] [T] 75.58%, [V] 54.80%  0.66\n",
      "[121:3] [T] 75.53%, [V] 55.85%  0.66\n",
      "[122:3] [T] 75.61%, [V] 55.05%  0.64\n",
      "[123:3] [T] 76.21%, [V] 55.45%  0.65\n",
      "[124:3] [T] 75.97%, [V] 55.50%  0.66\n",
      "[125:3] [T] 76.36%, [V] 55.40%  0.65\n",
      "[126:3] [T] 76.31%, [V] 55.50%  0.65\n",
      "[127:3] [T] 76.38%, [V] 54.95%  0.64\n",
      "[128:3] [T] 76.60%, [V] 55.20%  0.61\n",
      "[129:3] [T] 77.17%, [V] 55.35%  0.64\n",
      "[130:3] [T] 77.01%, [V] 54.55%  0.62\n",
      "[131:3] [T] 77.20%, [V] 55.25%  0.64\n",
      "[132:3] [T] 77.39%, [V] 55.15%  0.64\n",
      "[133:3] [T] 77.60%, [V] 55.20%  0.60\n",
      "[134:3] [T] 77.61%, [V] 55.40%  0.60\n",
      "[135:3] [T] 77.95%, [V] 55.30%  0.59\n",
      "[136:3] [T] 78.16%, [V] 55.35%  0.61\n",
      "[137:3] [T] 78.38%, [V] 55.20%  0.60\n",
      "[138:3] [T] 78.46%, [V] 55.40%  0.61\n",
      "[139:3] [T] 78.61%, [V] 55.10%  0.59\n",
      "[140:3] [T] 79.00%, [V] 55.45%  0.56\n",
      "[141:3] [T] 79.00%, [V] 55.05%  0.59\n",
      "[142:3] [T] 79.20%, [V] 55.05%  0.59\n",
      "[143:3] [T] 79.22%, [V] 55.05%  0.59\n",
      "[144:3] [T] 79.60%, [V] 55.05%  0.59\n",
      "[145:3] [T] 79.54%, [V] 55.25%  0.57\n",
      "[146:3] [T] 79.64%, [V] 54.90%  0.56\n",
      "[147:3] [T] 79.62%, [V] 55.25%  0.59\n",
      "[148:3] [T] 79.81%, [V] 55.15%  0.57\n",
      "[149:3] [T] 79.71%, [V] 54.80%  0.59\n",
      "[150:3] [T] 80.14%, [V] 55.10%  0.58\n",
      "[151:3] [T] 80.14%, [V] 55.15%  0.56\n",
      "[152:3] [T] 80.22%, [V] 54.85%  0.58\n",
      "[153:3] [T] 80.24%, [V] 54.70%  0.56\n",
      "[154:3] [T] 80.61%, [V] 54.80%  0.57\n",
      "[155:3] [T] 80.65%, [V] 54.95%  0.55\n",
      "[156:3] [T] 80.86%, [V] 54.85%  0.54\n",
      "[157:3] [T] 81.17%, [V] 55.15%  0.54\n",
      "[158:3] [T] 80.99%, [V] 54.95%  0.54\n",
      "[159:3] [T] 81.19%, [V] 55.20%  0.54\n",
      "[160:3] [T] 81.40%, [V] 55.00%  0.52\n",
      "[161:3] [T] 81.47%, [V] 55.25%  0.55\n",
      "[162:3] [T] 81.36%, [V] 55.25%  0.53\n",
      "[163:3] [T] 81.69%, [V] 55.30%  0.55\n",
      "[164:3] [T] 81.96%, [V] 55.25%  0.53\n",
      "[165:3] [T] 82.11%, [V] 54.95%  0.53\n",
      "[166:3] [T] 82.33%, [V] 54.90%  0.52\n",
      "[167:3] [T] 82.44%, [V] 54.85%  0.50\n",
      "[168:3] [T] 82.55%, [V] 54.70%  0.52\n",
      "[169:3] [T] 82.72%, [V] 55.15%  0.50\n",
      "[170:3] [T] 82.81%, [V] 55.35%  0.52\n",
      "[171:3] [T] 83.04%, [V] 54.85%  0.52\n",
      "[172:3] [T] 83.19%, [V] 55.05%  0.49\n",
      "[173:3] [T] 83.26%, [V] 55.40%  0.50\n",
      "[174:3] [T] 83.11%, [V] 55.15%  0.52\n",
      "[175:3] [T] 83.17%, [V] 54.90%  0.48\n",
      "[176:3] [T] 83.66%, [V] 55.35%  0.46\n",
      "[177:3] [T] 83.76%, [V] 54.70%  0.48\n",
      "[178:3] [T] 84.15%, [V] 55.25%  0.49\n",
      "[179:3] [T] 83.94%, [V] 55.25%  0.48\n",
      "[180:3] [T] 84.10%, [V] 54.95%  0.49\n",
      "[181:3] [T] 84.04%, [V] 54.95%  0.47\n",
      "[182:3] [T] 84.26%, [V] 55.05%  0.47\n",
      "[183:3] [T] 84.41%, [V] 55.10%  0.46\n",
      "[184:3] [T] 84.70%, [V] 54.95%  0.47\n",
      "[185:3] [T] 84.72%, [V] 55.10%  0.47\n",
      "[186:3] [T] 84.84%, [V] 54.65%  0.46\n",
      "[187:3] [T] 84.90%, [V] 54.95%  0.47\n",
      "[188:3] [T] 85.05%, [V] 55.10%  0.45\n",
      "[189:3] [T] 85.25%, [V] 55.05%  0.46\n",
      "[190:3] [T] 85.16%, [V] 55.35%  0.47\n",
      "[191:3] [T] 85.46%, [V] 55.25%  0.46\n",
      "[192:3] [T] 85.45%, [V] 55.35%  0.44\n",
      "[193:3] [T] 85.91%, [V] 54.75%  0.44\n",
      "[194:3] [T] 85.81%, [V] 54.90%  0.44\n",
      "[195:3] [T] 85.92%, [V] 55.00%  0.45\n",
      "[196:3] [T] 86.15%, [V] 54.10%  0.43\n",
      "[197:3] [T] 86.15%, [V] 54.95%  0.43\n",
      "[198:3] [T] 86.29%, [V] 54.90%  0.43\n",
      "[199:3] [T] 86.42%, [V] 54.70%  0.42\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        total   = 0\n",
    "        for i, (*cat_var_list, num_var, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            cat_var_list   = [t.to(device) for t in cat_var_list]\n",
    "            num_var        = num_var.to(device)\n",
    "            y              = y.to(device)\n",
    "            cat_variables  = dict(zip(column_order, cat_var_list))\n",
    "            res            = model(cat_variables, num_var)\n",
    "            loss           = criterion(res, y)\n",
    "            correct       += (res.argmax(dim = 1) == y).detach().sum().item()\n",
    "            total         += y.shape[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             if i % 50 == 0:\n",
    "#                 print(f'[{epoch}:{i}] {loss.item()}')\n",
    "        model.eval()\n",
    "        valid_correct = 0\n",
    "        valid_total   = 0\n",
    "        with torch.no_grad():\n",
    "            for *cat_var_list, num_var, y in valid_loader:\n",
    "                cat_var_list   = [t.to(device) for t in cat_var_list]\n",
    "                num_var        = num_var.to(device)\n",
    "                y              = y.to(device)\n",
    "                cat_variables  = dict(zip(column_order, cat_var_list))\n",
    "                res            = model(cat_variables, num_var)\n",
    "                valid_correct += (res.argmax(dim = 1) == y).detach().sum().item()\n",
    "                valid_total   += y.shape[0]\n",
    "        print(f'[{epoch}:{i}] [T] {100. * correct / total:5.2f}%, [V] {100. * valid_correct / valid_total:5.2f}% {loss.item():5.2f}')\n",
    "        model.train()\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.547\n"
     ]
    }
   ],
   "source": [
    "model = EdlvlClassifier(\n",
    "    column_order,\n",
    "    columns_encoders,\n",
    "    {\n",
    "        col : emb_dim\n",
    "        for col in columns_encoders\n",
    "    },\n",
    "    len(target_encoder),\n",
    "    num_var_number = df_num.shape[1]\n",
    "\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "valid_correct = 0\n",
    "valid_total   = 0\n",
    "with torch.no_grad():\n",
    "    for *cat_var_list, num_var, y in valid_loader:\n",
    "        cat_var_list   = [t.to(device) for t in cat_var_list]\n",
    "        num_var        = num_var.to(device)\n",
    "        y              = y.to(device)\n",
    "        cat_variables  = dict(zip(column_order, cat_var_list))\n",
    "        res            = model(cat_variables, num_var)\n",
    "        valid_correct += (res.argmax(dim = 1) == y).detach().sum().item()\n",
    "        valid_total   += y.shape[0]\n",
    "print(valid_correct / valid_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_idx = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *cat_var_list, num_var, y = train_dataset[sample_idx]\n",
    "# cat_var_list   = [t.to(device) for t in cat_var_list]\n",
    "# num_var        = num_var.to(device)\n",
    "# y              = y.to(device)\n",
    "# cat_variables  = dict(zip(column_order, [t.unsqueeze(0) for t in cat_var_list]))\n",
    "# res            = model(cat_variables, num_var.unsqueeze(0))\n",
    "# print(res.argmax(dim = 1).item(), y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot(index, res, x_label, correct_index = None):\n",
    "#     fig, ax = plt.subplots(figsize = (15, 10))\n",
    "#     bar_width = .70\n",
    "#     if correct_index is None:\n",
    "#         ax.bar(index, res, bar_width)\n",
    "#     else:\n",
    "#         barlist = ax.bar(index, res, bar_width)\n",
    "#         barlist[correct_index].set_color('r')\n",
    "#     ax.set_xticks(index)\n",
    "#     ax.set_xticklabels(index)\n",
    "#     ax.set_title(f'Prediction confidence by {x_label} on a specific sample')\n",
    "#     ax.set_xlabel(x_label)\n",
    "#     ax.set_ylabel('Confidence')\n",
    "    \n",
    "# plot(np.arange(len(res[0])), torch.softmax(res, dim = 1).detach().cpu().numpy()[0], 'pred_class', y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *cat_var_list, num_var, y = train_dataset[sample_idx]\n",
    "# cat_var_list   = [t.to(device) for t in cat_var_list]\n",
    "# num_var        = num_var.to(device)\n",
    "# y              = y.to(device)\n",
    "# cat_variables  = dict(zip(column_order, [t.unsqueeze(0) for t in cat_var_list]))\n",
    "# res            = model(cat_variables, num_var.unsqueeze(0))\n",
    "# print(y.item(), res.argmax(dim = 1).item())\n",
    "# print(cat_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *cat_var_list, num_var, y = train_dataset[sample_idx]\n",
    "# cat_var_list   = [t.to(device) for t in cat_var_list]\n",
    "# num_var        = num_var.to(device)\n",
    "# y              = y.to(device)\n",
    "# cat_variables  = dict(zip(column_order, [t.unsqueeze(0) for t in cat_var_list]))\n",
    "# orig_age_lvl   = cat_variables['AGELVL'].item()\n",
    "# confidences = []\n",
    "# age_lvls = list(range(12))\n",
    "# for age_lvl in age_lvls:\n",
    "#     cat_variables['AGELVL'][0] = age_lvl\n",
    "#     res = model(cat_variables, num_var.unsqueeze(0))\n",
    "#     res = torch.softmax(res, dim = 1)\n",
    "#     confidences.append(res[0, y].item())\n",
    "\n",
    "# plot(age_lvls, confidences, 'AGELVL', orig_age_lvl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_both_figures(sample_idx, col, bar_width = 0.7, figsize = (25, 12)):\n",
    "    *cat_var_list, num_var, y = train_dataset[sample_idx]\n",
    "    cat_var_list              = [t.to(device) for t in cat_var_list]\n",
    "    num_var                   = num_var.to(device)\n",
    "    y                         = y.to(device)\n",
    "    cat_variables             = dict(zip(column_order, [t.unsqueeze(0) for t in cat_var_list]))\n",
    "    pred                      = model(cat_variables, num_var.unsqueeze(0))\n",
    "    print(f'target: {y.item()}, model pred: {pred.argmax(dim = 1).item()}')\n",
    "    orig_col_value            = cat_variables[col].item()\n",
    "    confidences               = []\n",
    "    col_values                = list(range(len(df[col].unique())))\n",
    "    print(cat_variables)\n",
    "    for col_val in col_values:\n",
    "        cat_variables[col][0] = col_val\n",
    "        res = model(cat_variables, num_var.unsqueeze(0))\n",
    "        res = torch.softmax(res, dim = 1)\n",
    "#        confidences.append(res[0, y].item())\n",
    "        confidences.append(res.max())\n",
    "    \n",
    "    fig, ax = plt.subplots(\n",
    "        nrows = 1, \n",
    "        ncols = 2, \n",
    "        figsize = figsize\n",
    "    )\n",
    "    class_index = list(range(len(df_target.unique())))\n",
    "    ax[0].set_xticks(class_index)\n",
    "    ax[0].set_xticklabels(class_index)\n",
    "    barlist = ax[0].bar(class_index, torch.softmax(pred, dim = 1).detach().cpu().numpy()[0], bar_width)\n",
    "    barlist[y.item()].set_color('r')\n",
    "    ax[0].set_title(f'Model prediction confidence by class')\n",
    "    ax[0].set_xlabel(col_desc[df_target.name])\n",
    "    ax[0].set_ylabel('Confidence')\n",
    "    \n",
    "    ax[1].set_xticks(col_values)\n",
    "    ax[1].set_xticklabels(col_values)\n",
    "    barlist = ax[1].bar(col_values, confidences, bar_width)\n",
    "    barlist[orig_col_value].set_color('r')\n",
    "    ax[1].set_title(f'Model prediction confidence while varying a column')\n",
    "    ax[1].set_xlabel(col_desc[col])\n",
    "    ax[1].set_ylabel('Confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: 5, model pred: 5\n",
      "{'AGELVL': tensor([0], device='cuda:0'), 'AGYSUB': tensor([82], device='cuda:0'), 'EDLVL': tensor([0], device='cuda:0'), 'GSEGRD': tensor([3], device='cuda:0'), 'LOC': tensor([16], device='cuda:0'), 'LOSLVL': tensor([2], device='cuda:0'), 'OCC': tensor([98], device='cuda:0'), 'PATCO': tensor([0], device='cuda:0'), 'PPGRD': tensor([136], device='cuda:0'), 'STEMOCC': tensor([1], device='cuda:0'), 'SUPERVIS': tensor([0], device='cuda:0'), 'TOA': tensor([3], device='cuda:0'), 'WORKSCH': tensor([3], device='cuda:0'), 'WORKSTAT': tensor([1], device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_both_figures(73, 'WORKSCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdlvlClassifier(\n",
       "  (cat_emb): CategoricalEmbeddings(\n",
       "    (cat_embs): ModuleDict(\n",
       "      (AGELVL): Embedding(11, 5)\n",
       "      (AGYSUB): Embedding(351, 5)\n",
       "      (EDLVL): Embedding(21, 5)\n",
       "      (GSEGRD): Embedding(16, 5)\n",
       "      (LOC): Embedding(78, 5)\n",
       "      (LOSLVL): Embedding(10, 5)\n",
       "      (OCC): Embedding(459, 5)\n",
       "      (PATCO): Embedding(7, 5)\n",
       "      (PPGRD): Embedding(384, 5)\n",
       "      (STEMOCC): Embedding(86, 5)\n",
       "      (SUPERVIS): Embedding(6, 5)\n",
       "      (TOA): Embedding(15, 5)\n",
       "      (WORKSCH): Embedding(7, 5)\n",
       "      (WORKSTAT): Embedding(2, 5)\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=71, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.0)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
